\chapter{Modelo y propuesta de Investigación}

\section{Modelo de Investigación para la estimación de parámetros morfológicos en rocas sedimentarias usando Fourier Elíptico y redes neuronales}

En la Figura~\ref{fig:fig14} se describen las etapas de investigación, para después ser detalladas.

\begin{itemize}
	\item La primera etapa consiste en conseguir 1123 imágenes de todas las clases de esfericidad y redondez para poder entrenar de manera balanceada la red, y después probar con las imágenes de Krumbein \cite{Krumbein1941} para verificar las mediciones.
	
	\item La segunda etapa se obtiene el valor de redondez y esfericidad de cada una de las imágenes de entrenamiento con los métodos propuestos.
	\item La tercera etapa consiste en calcular los primeros 40 armónicos de la serie de Fourier Elíptico de cada una de las imágenes de entrenamiento y relacionar estos armónicos con su valor de esfericidad y redondez.
	\begin{figure}[hbtp]
		\centering 
		\includegraphics[angle=90,origin=c,scale=.55]{figuras/diagramaFlujoTrabajo.png}
		\caption{Modelo metodológico de la estimación de parámetros morfológicos en rocas sedimentarias usando Fourier Elíptico y redes neuronales}
		\label{fig:fig14}
	\end{figure}
	\item La cuarta etapa consiste entrenar las 2 redes neuronales, la que clasifica la esfericidad, y la de la redondez, con los valores de entrada que serán los armónicos de Fourier Elíptico  y la su respectiva salida.
	\item Una vez entrenadas las redes, se prueban utilizando armónicos de Fourier Elíptico del conjunto de imágenes de prueba.
	
	\item La última etapa es medir el error y clasificar las imágenes de prueba y observar los resultados.
\end{itemize}

\section{Fourier Elíptico}

El análisis de Fourier ha sido utilizado para caracterizar contornos cerrados. Fourier Elíptico es una extensión de Fourier clásico el cual simplifica la estimación de los coeficientes a través del código de cadena del contorno cerrado. Los coeficientes correspondientes a la magnitud son invariantes a la escala, rotación y traslación.

El código de cadena es el primer paso, inicialmente  descrito por Freeman\cite{freeman1974computer}, aproximando un contorno cerrado por una secuencia de trayectorias con 8 posibles valores. El código de un contorno es la cadena \(V\) de longitud \(K\):
\begin{equation}
	V = a_1a_2a_3...a_K,
\end{equation}
donde cada unión \(a_i\) es un entero del 0 al 7 orientado en la dirección \((\frac{\pi}{r})a_i\) \cite{Kuhl1982}.
%Aclarar este parrafo
En la Figura~\ref{fig:fig15} se puede observar como un contorno cerrado separado en píxeles, se puede obtener el código de cadena, trazando una trayectoria desde un punto inicial, hasta volver a llegar a ese mismo punto, pasando por todo el contorno. El código de cadena de la Figura~\ref{fig:fig15}a iniciando de el extremo superior izquierdo es:
\begin{equation}
	V = 0005676644422123
\end{equation}
\begin{figure}[H]
	\centering 
	\includegraphics[scale=1]{figuras/codigoCadena.png}
	\caption{Representación gráfica del código de cadena de un contorno cerrado \cite{Kuhl1982}}
	\label{fig:fig15}
\end{figure}

Debido a que el cambio en el código de cadena esa constante, los coeficientes pueden ser encontrados más fácilmente. Por lo que los coeficientes de la componente \(x\) son:
\begin{equation}
	a_n = \frac{T}{2n^2\pi^2}\sum_{p = 1}^{K}\frac{\Delta x_p}{\Delta t_p}[\cos{\frac{2n\pi t_p}{T}} - \cos{\frac{2n\pi t_{p-1}}{T}}]
\end{equation}
\begin{equation}
	b_n = \frac{T}{2n^2\pi^2}\sum_{p = 1}^{K}\frac{\Delta x_p}{\Delta t_p}[\sin{\frac{2n\pi t_p}{T}} - \sin{\frac{2n\pi t_{p-1}}{T}}]
\end{equation}
donde, la K es el número de píxeles del contorno, \(T\) el período fundamental, \(\Delta x_p\) el cambio en el eje de las x, \(\Delta t_p\) el cambio en el tiempo.

Con eso obtendríamos los coeficientes de la componente \(x\), pero como una imagen es una señal en dos dimensiones, además el cambio de la \(x\) no siempre sera igual al cambio en las \(y\), necesitamos también obtener los coeficientes de la componente \(y\).
\begin{equation}
	c_n = \frac{T}{2n^2\pi^2}\sum_{p = 1}^{K}\frac{\Delta y_p}{\Delta t_p}[\cos{\frac{2n\pi t_p}{T}} - \cos{\frac{2n\pi t_{p-1}}{T}}]
\end{equation}
\begin{equation}
	d_n = \frac{T}{2n^2\pi^2}\sum_{p = 1}^{K}\frac{\Delta y_p}{\Delta t_p}[\sin{\frac{2n\pi t_p}{T}} - \sin{\frac{2n\pi t_{p-1}}{T}}]
\end{equation}
Las cuatro expresiones anteriores conforman los coeficientes de Fourier elíptico. Cabe notar que su nombre se debe a que genera fasores elípticos en lugar de circulares como el método tradicional.

Para que este método obtenga la invarianza a la escala, rotación y traslación, es necesario aplicar una normalización y ajustar los ángulos en los que va a iniciar cada elipse, para que independientemente de estas tres características del contorno cerrado siempre de el mismo resultado. La expresión para normalizar es la siguiente
\begin{equation}
	E_p = ((A_0-x_p)^2 + (C_0-y_p)^2)^\frac{1}{2}
\end{equation}
donde \(A_0\) y \(C_0\) son el promedio de la energía de las componentes \(x\) y \(y\) respectivamente. Para obtener el ángulo de rotación inicial \(\theta_p\) a el índice \(p\):
\begin{equation}
	\theta_p = \frac{2\pi t_p}{T}, 0<\theta_p\le2\pi
\end{equation}
y para obtener el ángulo de rotación espacial \(\psi_p\)
\begin{equation}
	\psi_p=\arctan[\frac{y_p-C_0}{x_p-A_0}], 0\le\psi_p<2\pi
\end{equation}
Al iniciar el cálculo de los coeficientes, se tendría que obtener \(E_1\), \(\theta_1\) y \(\psi_1\) para influir en los siguiente coeficientes a que se reajusten de acuerdo a estos ángulos iniciales, y dividirlo entre \(E_0\) para mantener la invarianza a la escala, por lo que la obtención de los nuevos coeficientes sería:
\begin{equation}
	\begin{bmatrix}
	1a^{**}_n & 1b^{**}_n\\
	1c^{**}_n & 1d^{**}_n
	\end{bmatrix} =
	\begin{bmatrix}
	\cos{\psi_1} & \sin{\psi_1}\\
	-\sin{\psi_1} & \cos{\psi_1}
	\end{bmatrix}
	\begin{bmatrix}
	a_n & b_n\\
	c_n & d_n
	\end{bmatrix}
	\begin{bmatrix}
	\cos{n\theta_1} & -\sin{n\theta_1}\\
	\sin{n\theta_1} & \cos{n\theta_1}
	\end{bmatrix}
\end{equation}
\begin{equation}
	\begin{bmatrix}
	2a^{**}_n & 2b^{**}_n\\
	2c^{**}_n & 2d^{**}_n
	\end{bmatrix} =
	(-1)^{n+1}
	\begin{bmatrix}
	1a^{**}_n & 1b^{**}_n\\
	1c^{**}_n & 1d^{**}_n
	\end{bmatrix}
\end{equation}
El resultado de la ecuación 3.11 nos daría los coeficientes \(a,b,c,d\) para el \(n\) armónico que se está calculando. Siendo invariante a la escala, rotación y traslación. Los detalles pueden ser consultados en \cite{Kuhl1982}.
\section{Algoritmo para estimar la redondez}

El término de redondez es una característica morfológica más compleja que la forma. Se dice que es de segundo orden porque esta superpuesta a la forma, esto la hace independiente. Como mencionamos anteriormente, la redondez se medirá mediante la métrica propuesta de Wadell \cite{Wadell1935} que consiste en identificar las principales curvaturas (esquinas) del contorno. El concepto es simple pero su algoritmo es complejo debido a que el número y grado de curvatura depende del tamaño del contorno. Proponemos utilizar el algoritmo desarrollado por Zheng \cite{Zheng2016} el cual detallamos en seguida. 

El primer bloque consiste en obtener el radio del máximo círculo circunscrito de la partícula. Siguiendo la Figura~\ref{fig:circunFlujo}, el paso a) es tener nuestra imagen de la partícula en binario, en el paso b) es transformar nuestra imagen en un mapa de distancias euclidianas, una matriz que nos indica que tan retirado esta un píxel de un contorno cerrado, entre más distancia, mayor será el valor; y por último, en el paso c), seleccionamos el píxel más retirado del contorno de la partícula como el centro del círculo, b) y su distancia euclidiana será el radio. 

\begin{figure}[H]
	\centering 
	\includegraphics[scale=.42]{figuras/fig3_1.png}
	\caption{Flujo de trabajo para obtener el mayor círculo circunscrito.}
	\label{fig:circunFlujo}
\end{figure}

Al haber obtenido el mayor círculo circunscrito, ahora se tiene que trazar un círculo que se ajuste a cada una de las esquinas, pero antes de eso, se tiene que suavizar el contorno de la partícula, para evitar que la información de la rugosidad afecte con el ajuste de los círculos. En el artículo de Zheng \cite{Zheng2016} usan la regresión \textit{LOESS} y \textit{k-folds} para suavizar la partícula, pero nosotros decidimos usar Fourier Elíptico solo tomando en cuenta los primeros 30 armónicos de la serie por la facilidad que resulta quitar la información de la rugosidad.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=.42]{figuras/fig3_2.png}
	\caption{Resultado del suavizado de la partícula utilizando Fourier Elíptico.}
	\label{fig:suavFourier}
\end{figure}

En la figura~\ref{fig:suavFourier}, se observa el suavizado generado por Fourier Elíptico. Para poder identificar las esquinas de la partícula, se necesita analizar todo el contorno de la partícula iniciando desde cualquier punto e ir analizando que la sucesión de puntos tenga un valor de curvatura positiva, de esta manera se puede discriminar cuando esa curvatura se encuentra por fuera de la partícula y solo dejando las que están por dentro. Para formar los círculos, se utiliza una distancia máxima la cual regula que tan retirados deben de estar los píxeles del contorno para ser considerados como una sola esquina para después ajustar un círculo a todos esos puntos.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=.42]{figuras/fig3_3.png}
	\caption{Flujo para aproximar las esquinas con círculos.}
	\label{fig:flujoCirculos}
\end{figure}

En la Figura~\ref{fig:circunFlujo} se muestra como obtener los círculos. (a) tenemos el contorno de la partícula; (b) remarcamos las  partes del contorno que podrían ser esquinas utilizando geometría computacional; (c) se discriminan regiones segúns su curvatura y longitud, así se seleccionan las esquinas y se ajusta un círculo (radio de curvatura) para representarlas.
\begin{equation}
	\label{eqn:roundness}
	\frac{\sum{\frac{r}{R}}}{N} = \text{Grado de redondez}
\end{equation}
Una vez obtenido lo anterior, se pasa a calcular el grado de redondez usando la ecuación ~\ref{eqn:roundness}. El numerador es el promedio de los radios de todos los círculos de las esquinas y el denominador corresponde al radio del círculo circunscrito más grande en la partícula. El resultado de está relación es un valor entre 0 y 1, como se describe en el artículo de Zheng y Hryciw \cite{Zheng2016}.

El mayor inconveniente con este algoritmo es que tres parámetros dependen del tamaño de la partícula. Un valor mal seleccionado puede producir un error considerable en estimación de la redondez. Los mismo autores sugieren un método para sintonizar estos parámetros, sin embargo, en muchos casos se tiene que realizar una corrección manual. Esto reduce el número de imágenes en las que puede funcionar de manera no supervisada.
 
\section{Redes neuronales}

Una red neuronal artificial es un tipo de algoritmo de \textit{Machine Learning} el cual trata de simular el comportamiento del cerebro, al cual le llega una entrada, por medio de neuronas que se activan o no, se obtiene un resultado \cite{aggarwal2018neural}.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=.8]{figuras/redNeuronal.jpg}
	\caption{Estructura de una neurona artificial análogo a una biológica.}
	\label{fig:estructuraNeurona}
\end{figure}

La neurona es la unidad básica, posee dos tareas las cuales son combinar entrada y producir la señal de activación, siendo un nodo en un grafo dirigido (Red neuronal artificial). La conexión entre 2 neuronas es conocida como la sinapsis y su fuerza esta determinada por el estímulo externo \cite{aggarwal2018neural}. 

\begin{figure}[H]
	\centering 
	\includegraphics[scale=.5]{figuras/ANN.png}
	\caption{Arquitectura de una red neuronal artificial.}
	\label{fig:arqRedNeuronal}
\end{figure}

Las conexiones o aristas están regidas por pesos (\(w_{ij}\)), esos pesos se mezclan con las entradas para producir el "estímulo", todos los estímulos de entrada hacia una neurona se combinan para después ingresarlas a la función de activación (\(f()\)) que determinará la salida hacia la siguiente neurona. En la Figura~\ref{fig:estructuraNeurona} se puede observar los elementos relacionados a una neurona de una red neuronal artificial. 

En la Figura~\ref{fig:arqRedNeuronal} se observa la arquitectura de una red neuronal con su capa de entrada con 4 neuronas, 4 capas ocultas con 7 neuronas cada una, y su capa de salida con 4 neuronas.

\subsection{Funciones de Activación}

Las funciones de activación tienen como objetivo el acotar los valores de salida de una neurona a un cierto rango de valores. La selección de las funciones de activación dependerá del problema con el cual se este manejando. Existen funciones lineales y no lineales. Las lineales tienen un uso exclusivo, cuando el problema se trata de regresión y solamente en la capa de salida \cite{ding2018activation}.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=.6]{figuras/Sigmoid.png}
	\caption{Función de activación Sigmoide.}
	\label{fig:sigmoid}
\end{figure}
\useshortskip
\begin{equation}
	\label{eqn:sigmoid}
	g(x)= \sigma(x)= \frac{1}{1+e^{-x}} \;\;\;\;\;\;\;\; g'(x)= \sigma(x)(1-\sigma(x))
\end{equation} 
La primer función de activación que surgió fue la Sigmoide, representada en la Figura ~\ref{fig:sigmoid} y expresada por la función \(g(x)\) y su derivada \(g'(x)\) ~\ref{eqn:sigmoid}. Utilizada principalmente para clasificar un conjunto de datos en 2 clases. La utilización de ella se ha visto mermada porque presenta dos grandes problemas cuando se utiliza como función de activación en las capas ocultas:

\begin{itemize}
	\item Asimetría positiva
	\item Desvanecimiento del gradiente
	\item Utilización de la función exponencial es costoso.
\end{itemize}

La asimetría positiva provoca que el gradiente se vuelva ineficiente en la búsqueda del mínimo, debido a que sólo puede tomar direcciones totalmente negativas o totalmente positivas, haciendo como una especie de zigzag hasta encontrar el punto mínimo. El desvanecimiento del gradiente se puede observar en la Figura ~\ref{fig:sigmoid}, ya que, a medida que los valores de x van incrementando, el valor de la derivada va teniendo a 0, provocando que no haya una retroalimentación a la hora de retropropagar hacia atrás, terminando en que se la red neuronal deje de aprender ya que sus pesos no se van a ir actualizando \cite{aggarwal2018neural,ding2018activation}.

La función se puede seguir usando en la capa de salida pero solo sí el problema lo requiere. Las funciones posteriores trataron de eliminar estos problemas antes mencionados

\begin{figure}[H]
	\centering 
	\includegraphics[scale=.6]{figuras/TanH.png}
	\caption{Función de activación Tangente Hiperbólico.}
	\label{fig:tanh}
\end{figure}
\useshortskip
\begin{equation}
	\label{eqn:tanh}
	g(x)= \tanh{x}=\frac{e^x - e^{-x}}{e^x + e^{-x}} \;\;\;\;\;\;\;\; g'(x)= 1 - \tanh^2{x}=\frac{4}{(e^x+e^{-x})^2}
\end{equation} 
La siguiente función que surgió fue la Tangente Hiperbólico (TanH), introducida por LeCun en 1991, representada en la Figura~\ref{fig:tanh}, expresada junto con su derivada en la Ecuación~\ref{eqn:tanh}. Al observar los problemas que presentaba la función Sigmoide, lo que se trataba de encontrar con la función TanH era eliminarlos, sin embargo, solo fue capaz de solucionar el problema de la asimetría positiva centrando los datos de -1 a 1, para que el descenso del gradiente fuera más eficiente. Sigue presentando los problemas de desvanecimiento del gradiente conforme los valores de \(x\) son más grandes, y sigue existiendo el alto costo por usar la función exponencial \cite{ding2018activation}.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=.6]{figuras/ReLU.png}
	\caption{Función de activación ReLU (Unidad Lineal Rectificada).}
	\label{fig:relu}
\end{figure}
\useshortskip
\begin{equation}
	\label{eqn:relu}
	g(x)= max(0,x) \;\;\;\;\;\;\;\; g'(x)= u(x)
\end{equation} 
La función ReLU o Unidad Lineal Rectificada fue introducida por Vinod Nair en 2010 \cite{nair2010rectified}, representada en la Figura~\ref{fig:relu}, expresada junto a su derivada en la Ecuación~\ref{eqn:relu}, la función \(u(x)\) es el escalón unitario. Nació para atacar el problema del desvanecimiento del gradiente, pero sigue conservando, en una menor magnitud, que las otras 2 funciones. Sigue poseyendo el problema de la asimetría positiva por no centrar los datos, y se corre el riesgo de que partes de la red neuronal se desconecten si la función empieza a enviar puros ceros, pero tiene la ventaja de que tiene un costo bastante bajo \cite{ding2018activation}.

Fue muy popular años más tarde de su nacimiento, pero por los problemas que siguieron existiendo se trató de buscar una variante la cual controlara o eliminara por completo los problemas, son las siguientes 2 funciones de activación.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=.6]{figuras/PReLU.png}
	\caption{Función de activación PReLU (Unidad Lineal Rectificada Parametrizada).}
	\label{fig:prelu}
\end{figure}
\useshortskip
\begin{equation}
	\label{eqn:prelu}
	g(x)= max(\alpha x,x) \;\;\;\;\;\;\;\; g'(x) = \alpha + (1-\alpha)u(x)
\end{equation} 
La función PReLU o Unidad Lineal Rectificada Parametrizada fue introducida por Xiangyu Zhang en el año 2015 \cite{he2015delving}, representada en la Figura~\ref{fig:prelu}, expresada junto a su derivada en la Ecuación~\ref{eqn:prelu}. El parámetro \(\alpha\) es un coeficiente que va irse adaptando y aprendiendo a lo largo del proceso de aprendizaje de la red neuronal, promoviendo la rapidez del aprendizaje. Ataca principalmente el desvanecimiento del gradiente, ya que la derivada no sería cero, y estaría habiendo retroalimentación en la red, evitando que se desconecten partes de la misma, como a su vez logra reducir la asimetría positiva. La función Leaky ReLU es un caso particular de está función en la cual se iguala \(\alpha\) a un valor específico, regularmente es .1 \cite{ding2018activation}.

\begin{figure}[H]
	\centering 
	\includegraphics[scale=.6]{figuras/ELU.png}
	\caption{Función de activación ELU (Unidad Lineal Exponencial).}
	\label{fig:elu}
\end{figure}
\useshortskip
\begin{equation}
	\label{eqn:elu}
	g(x)=
	\begin{cases}
	x & x\ge 0\\
	e^x-1 & x<0
	\end{cases}
	\;\;\;\;\;\;\;\;
	g'(x)=
	\begin{cases}
	1 & x\ge 0\\
	e^x & x<0
	\end{cases}
\end{equation} 
Por último, se tiene la función ELU o Unidad Lineal Exponencial introducido por Djork-Arne Clevert en 2016 \cite{clevert2015fast}. Una gran mejora a comparación de ReLU, ya que no sufre del problema de que se desconecten partes de la red, ataca de mejor manera la asimetría positiva, mejora la velocidad de aprendizaje de la red neuronal, a pesar de utilizar la función exponencial y aumentar el tiempo de procesamiento, es un buen trato por obtener una buena función de activación \cite{ding2018activation}.

Para este trabajo, se tomaron en cuenta las funciones PReLU y ELU por ser las más confiables y las que poseen menos problemas, solamente para las capas ocultas; pero, para el caso de la redondez se usó la función sigmoide como se describe en el capítulo 4. La capa de salida se tomó en cuenta una función lineal de la esfericidad, para verlo como un problema de regresión al igual que para la redondez. 

Los pesos de cada conexión entre neuronas se actualiza al final de cada época, ese valor esta definido por una función optimizadora que se necesita una velocidad de aprendizaje o \textit{"learning rate"} y una función de error que calcula que tan errónea fue la salida de la red neuronal con respecto al valor original, si la velocidad de aprendizaje es muy alta, nunca va a encontrar el punto mínimo debido a que siempre se lo va a pasar y regresar una y otra vez, si el valor es muy pequeño, la función tardaría demasiado en llegar y quizás nunca converja \cite{aggarwal2018neural}.

Actualmente existen funciones las cuales se les puede asignar un valor de aprendizaje alto pero a su vez asignar un valor de caída del aprendizaje, haciendo que en las época iniciales sea muy rápido pero su velocidad vaya bajando gradualmente para ayudar en la convergencia. Tal es el caso de la función RMSprop, que se describen en las siguientes ecuaciones: %~\ref{eqn:rmsprop} están sus ecuaciones.
\begin{equation}
	V_{dw}=\beta\cdot V_{dw}+(1-\beta)\cdot dw^2 
\end{equation}
\begin{equation}
	V_{db}=\beta\cdot V_{dw}+(1-\beta)\cdot db^2
\end{equation}
\begin{equation}
	W = W - \alpha\cdot \frac{dw}{\sqrt{V_{dw}}+\epsilon} 
\end{equation}
\begin{equation}
	b = b - \alpha\cdot \frac{db}{\sqrt{V_{db}}+\epsilon}
\end{equation}
La función de error ayuda a la de optimización a medir el error que hay entre el resultado obtenido y el real, de tal manera que se sepa que tanto se tienen que actualizar los pesos para ir reduciendo el error lo más posible.

Una época esta definida por la ejecución de cierto flujo de pasos, inicia al ingresar el primer registro de los datos, después actualizar los pesos en base al error, así hasta acabar con cada uno de los registros, esa es la duración de una época.



%\begin{equation}
%\begin{split}
%\label{eqn:rmsprop}
%V_{dw}=\beta\cdot V_{dw}+(1-\beta)\cdot dw^2 \\
%%V_{db}=\beta\cdot V_{dw}+(1-\beta)\cdot db^2 \\
%W = W - \alpha\cdot \frac{dw}{\sqrt{V_{dw}}+\epsilon} \\
%b = b - \alpha\cdot \frac{db}{\sqrt{V_{db}}+\epsilon}
%\end{split}
%\end{equation}